<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv Sanity Random Papers Feed (Last Week)</title><description>Random papers from last week.</description><link>https://hedgehog.den.dev/feeds/random-last-week.xml</link><atom:link href="https://hedgehog.den.dev/feeds/random-last-week.xml" rel="self" type="application/rss+xml" /><item><title>EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video
  Grounding with Multimodal Large Language Model</title><dc:creator>Guozhang Li</dc:creator><dc:creator>Xinpeng Ding</dc:creator><dc:creator>De Cheng</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><description>Early weakly supervised video grounding (WSVG) methods often struggle with
incomplete boundary detection due to the absence of temporal boundary
annotations. To bridge the gap between video-level and boundary-level
annotation, explicit-supervision methods, i.e., generating pseudo-temporal
boundaries for training, have achieved great success. However, data
augmentations in these methods might disrupt critical temporal information,
yielding poor pseudo boundaries. In this paper, we propose a new perspective
that maintains the integrity of the original temporal content while introducing
more valuable information for expanding the incomplete boundaries. To this end,
we propose EtC (Expand then Clarify), first use the additional information to
expand the initial incomplete pseudo boundaries, and subsequently refine these
expanded ones to achieve precise boundaries. Motivated by video continuity,
i.e., visual similarity across adjacent frames, we use powerful multimodal
large language models (MLLMs) to annotate each frame within initial pseudo
boundaries, yielding more comprehensive descriptions for expanded boundaries.
To further clarify the noise of expanded boundaries, we combine mutual learning
with a tailored proposal-level contrastive objective to use a learnable
approach to harmonize a balance between incomplete yet clean (initial) and
comprehensive yet noisy (expanded) boundaries for more precise ones.
Experiments demonstrate the superiority of our method on two challenging WSVG
datasets.</description><link>https://arxiv.org/abs/2312.02483</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02483</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Fast Dual Subgradient Optimization of the Integrated Transportation
  Distance Between Stochastic Kernels</title><dc:creator>Zhengqi Lin</dc:creator><dc:creator>Andrzej Ruszczynski</dc:creator><description>A generalization of the Wasserstein metric, the integrated transportation
distance, establishes a novel distance between probability kernels of Markov
systems. This metric serves as the foundation for an efficient approximation
technique, enabling the replacement of the original system's kernel with a
kernel with a discrete support of limited cardinality. To facilitate practical
implementation, we present a specialized dual algorithm capable of constructing
these approximate kernels quickly and efficiently, without requiring
computationally expensive matrix operations. Finally, we demonstrate the
efficacy of our method through several illustrative examples, showcasing its
utility in practical scenarios. This advancement offers new possibilities for
the streamlined analysis and manipulation of stochastic systems represented by
kernels.</description><link>https://arxiv.org/abs/2312.01432</link><guid isPermaLink="true">https://arxiv.org/abs/2312.01432</guid><pubDate>Sun, 03 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Learning Content-enhanced Mask Transformer for Domain Generalized
  Urban-Scene Segmentation</title><dc:creator>Qi Bi</dc:creator><dc:creator>Shaodi You</dc:creator><dc:creator>Theo Gevers</dc:creator><description>Domain-generalized urban-scene semantic segmentation (USSS) aims to learn
generalized semantic predictions across diverse urban-scene styles. Unlike
domain gap challenges, USSS is unique in that the semantic categories are often
similar in different urban scenes, while the styles can vary significantly due
to changes in urban landscapes, weather conditions, lighting, and other
factors. Existing approaches typically rely on convolutional neural networks
(CNNs) to learn the content of urban scenes.
  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for
domain-generalized USSS. The main idea is to enhance the focus of the
fundamental component, the mask attention mechanism, in Transformer
segmentation models on content information. To achieve this, we introduce a
novel content-enhanced mask attention mechanism. It learns mask queries from
both the image feature and its down-sampled counterpart, as lower-resolution
image features usually contain more robust content information and are less
sensitive to style variations. These features are fused into a Transformer
decoder and integrated into a multi-resolution content-enhanced mask attention
learning scheme.
  Extensive experiments conducted on various domain-generalized urban-scene
segmentation datasets demonstrate that the proposed CMFormer significantly
outperforms existing CNN-based methods for domain-generalized semantic
segmentation, achieving improvements of up to 14.00\% in terms of mIoU (mean
intersection over union). The source code is publicly available at
\url{https://github.com/BiQiWHU/CMFormer}.</description><link>https://arxiv.org/abs/2307.00371</link><guid isPermaLink="true">https://arxiv.org/abs/2307.00371</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via
  Sim-to-real Adaptation</title><dc:creator>Geonhyup Lee</dc:creator><dc:creator>Joosoon Lee</dc:creator><dc:creator>Sangjun Noh</dc:creator><dc:creator>Minhwan Ko</dc:creator><dc:creator>Kangmin Kim</dc:creator><dc:creator>Kyoobin Lee</dc:creator><description>The study addresses the foundational and challenging task of peg-in-hole
assembly in robotics, where misalignments caused by sensor inaccuracies and
mechanical errors often result in insertion failures or jamming. This research
introduces PolyFit, representing a paradigm shift by transitioning from a
reinforcement learning approach to a supervised learning methodology. PolyFit
is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF
peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose
estimation and adjusts the peg pose to rectify misalignments. Extensive
training in a simulated environment involves a dataset encompassing a diverse
range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T
readings. To enhance extrinsic pose estimation, a multi-point contact strategy
is integrated into the model input, recognizing that identical F/T readings can
indicate different poses. The study proposes a sim-to-real adaptation method
for real-world application, using a sim-real paired dataset to enable effective
generalization to complex and unseen polygon shapes. PolyFit achieves
impressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen
shapes in simulations, respectively. Real-world evaluations further demonstrate
substantial success rates of 86.7% and 85.0%, highlighting the robustness and
adaptability of the proposed method.</description><link>https://arxiv.org/abs/2312.02531</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02531</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D
  Hybrid Prior</title><dc:creator>Xusen Sun</dc:creator><dc:creator>Longhao Zhang</dc:creator><dc:creator>Hao Zhu</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Bang Zhang</dc:creator><dc:creator>Xinya Ji</dc:creator><dc:creator>Kangneng Zhou</dc:creator><dc:creator>Daiheng Gao</dc:creator><dc:creator>Liefeng Bo</dc:creator><dc:creator>Xun Cao</dc:creator><description>Audio-driven talking head generation has drawn much attention in recent
years, and many efforts have been made in lip-sync, expressive facial
expressions, natural head pose generation, and high video quality. However, no
model has yet led or tied on all these metrics due to the one-to-many mapping
between audio and motion. In this paper, we propose VividTalk, a two-stage
generic framework that supports generating high-visual quality talking head
videos with all the above properties. Specifically, in the first stage, we map
the audio to mesh by learning two motions, including non-rigid expression
motion and rigid head motion. For expression motion, both blendshape and vertex
are adopted as the intermediate representation to maximize the representation
ability of the model. For natural head motion, a novel learnable head pose
codebook with a two-phase training mechanism is proposed. In the second stage,
we proposed a dual branch motion-vae and a generator to transform the meshes
into dense motion and synthesize high-quality video frame-by-frame. Extensive
experiments show that the proposed VividTalk can generate high-visual quality
talking head videos with lip-sync and realistic enhanced by a large margin, and
outperforms previous state-of-the-art works in objective and subjective
comparisons.</description><link>https://arxiv.org/abs/2312.01841</link><guid isPermaLink="true">https://arxiv.org/abs/2312.01841</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative
  Models on Medical Conversation Tasks</title><dc:creator>Ke Liang</dc:creator><dc:creator>Sifan Wu</dc:creator><dc:creator>Jiayi Gu</dc:creator><description>Using natural language processing (NLP) technologies to develop medical
chatbots makes the diagnosis of the patient more convenient and efficient,
which is a typical application in healthcare AI. Because of its importance,
lots of research have been come out. Recently, the neural generative models
have shown their impressive ability as the core of chatbot, while it cannot
scale well when directly applied to medical conversation due to the lack of
medical-specific knowledge. To address the limitation, a scalable Medical
Knowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism
aims to assist general neural generative models to achieve better performance
on the medical conversation task. The medical-specific knowledge graph is
designed within the mechanism, which contains 6 types of medical-related
information, including department, drug, check, symptom, disease, food.
Besides, the specific token concatenation policy is defined to effectively
inject medical information into the input data. Evaluation of our method is
carried out on two typical medical datasets, MedDG and MedDialog-CN. The
evaluation results demonstrate that models combined with our mechanism
outperform original methods in multiple automatic evaluation metrics. Besides,
MKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are
public:
https://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism</description><link>https://arxiv.org/abs/2312.02496</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02496</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Exploring Answer Information Methods for Question Generation with
  Transformers</title><dc:creator>Talha Chafekar</dc:creator><dc:creator>Aafiya Hussain</dc:creator><dc:creator>Grishma Sharma</dc:creator><dc:creator>Deepak Sharma</dc:creator><description>There has been a lot of work in question generation where different methods
to provide target answers as input, have been employed. This experimentation
has been mostly carried out for RNN based models. We use three different
methods and their combinations for incorporating answer information and explore
their effect on several automatic evaluation metrics. The methods that are used
are answer prompting, using a custom product method using answer embeddings and
encoder outputs, choosing sentences from the input paragraph that have answer
related information, and using a separate cross-attention attention block in
the decoder which attends to the answer. We observe that answer prompting
without any additional modes obtains the best scores across rouge, meteor
scores. Additionally, we use a custom metric to calculate how many of the
generated questions have the same answer, as the answer which is used to
generate them.</description><link>https://arxiv.org/abs/2312.03483</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03483</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>The Art of Camouflage: Few-shot Learning for Animal Detection and
  Segmentation</title><dc:creator>Thanh-Danh Nguyen</dc:creator><dc:creator>Anh-Khoa Nguyen Vu</dc:creator><dc:creator>Nhat-Duy Nguyen</dc:creator><dc:creator>Vinh-Tiep Nguyen</dc:creator><dc:creator>Thanh Duc Ngo</dc:creator><dc:creator>Thanh-Toan Do</dc:creator><dc:creator>Minh-Triet Tran</dc:creator><dc:creator>Tam V. Nguyen</dc:creator><description>Camouflaged object detection and segmentation is a new and challenging
research topic in computer vision. There is a serious issue of lacking data of
camouflaged objects such as camouflaged animals in natural scenes. In this
paper, we address the problem of few-shot learning for camouflaged object
detection and segmentation. To this end, we first collect a new dataset,
CAMO-FS, for the benchmark. We then propose a novel method to efficiently
detect and segment the camouflaged objects in the images. In particular, we
introduce the instance triplet loss and the instance memory storage. The
extensive experiments demonstrated that our proposed method achieves
state-of-the-art performance on the newly collected dataset.</description><link>https://arxiv.org/abs/2304.07444</link><guid isPermaLink="true">https://arxiv.org/abs/2304.07444</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of
  Illumination and Reflectance</title><dc:creator>Yuto Enyo</dc:creator><dc:creator>Ko Nishino</dc:creator><description>Reflectance bounds the frequency spectrum of illumination in the object
appearance. In this paper, we introduce the first stochastic inverse rendering
method, which recovers the full frequency spectrum of an illumination jointly
with the object reflectance from a single image. Our key idea is to solve this
blind inverse problem in the reflectance map, an appearance representation
invariant to the underlying geometry, by learning to reverse the image
formation with a novel diffusion model which we refer to as the Diffusion
Reflectance Map Network (DRMNet). Given an observed reflectance map converted
and completed from the single input image, DRMNet generates a reflectance map
corresponding to a perfect mirror sphere while jointly estimating the
reflectance. The forward process can be understood as gradually filtering a
natural illumination with lower and lower frequency reflectance and additive
Gaussian noise. DRMNet learns to invert this process with two subnetworks,
IllNet and RefNet, which work in concert towards this joint estimation. The
network is trained on an extensive synthetic dataset and is demonstrated to
generalize to real images, showing state-of-the-art accuracy on established
datasets.</description><link>https://arxiv.org/abs/2312.04529</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04529</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement
  Learning with General Function Approximation</title><dc:creator>Jiayi Huang</dc:creator><dc:creator>Han Zhong</dc:creator><dc:creator>Liwei Wang</dc:creator><dc:creator>Lin F. Yang</dc:creator><description>To tackle long planning horizon problems in reinforcement learning with
general function approximation, we propose the first algorithm, termed as
UCRL-WVTR, that achieves both \emph{horizon-free} and
\emph{instance-dependent}, since it eliminates the polynomial dependency on the
planning horizon. The derived regret bound is deemed \emph{sharp}, as it
matches the minimax lower bound when specialized to linear mixture MDPs up to
logarithmic factors. Furthermore, UCRL-WVTR is \emph{computationally efficient}
with access to a regression oracle. The achievement of such a horizon-free,
instance-dependent, and sharp regret bound hinges upon (i) novel algorithm
designs: weighted value-targeted regression and a high-order moment estimator
in the context of general function approximation; and (ii) fine-grained
analyses: a novel concentration bound of weighted non-linear least squares and
a refined analysis which leads to the tight instance-dependent bound. We also
conduct comprehensive experiments to corroborate our theoretical findings.</description><link>https://arxiv.org/abs/2312.04464</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04464</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment</title><dc:creator>Brian Gordon</dc:creator><dc:creator>Yonatan Bitton</dc:creator><dc:creator>Yonatan Shafir</dc:creator><dc:creator>Roopal Garg</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Dani Lischinski</dc:creator><dc:creator>Daniel Cohen-Or</dc:creator><dc:creator>Idan Szpektor</dc:creator><description>While existing image-text alignment models reach high quality binary
assessments, they fall short of pinpointing the exact source of misalignment.
In this paper, we present a method to provide detailed textual and visual
explanation of detected misalignments between text-image pairs. We leverage
large language models and visual grounding models to automatically construct a
training set that holds plausible misaligned captions for a given image and
corresponding textual explanations and visual indicators. We also publish a new
human curated test set comprising ground-truth textual and visual misalignment
annotations. Empirical results show that fine-tuning vision language models on
our training set enables them to articulate misalignments and visually indicate
them within images, outperforming strong baselines both on the binary alignment
classification and the explanation generation tasks. Our method code and human
curated test set are available at: https://mismatch-quest.github.io/</description><link>https://arxiv.org/abs/2312.03766</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03766</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Towards small and accurate convolutional neural networks for acoustic
  biodiversity monitoring</title><dc:creator>Serge Zaugg</dc:creator><dc:creator>Mike van der Schaar</dc:creator><dc:creator>Florence Erbs</dc:creator><dc:creator>Antonio Sanchez</dc:creator><dc:creator>Joan V. Castell</dc:creator><dc:creator>Emiliano Ramallo</dc:creator><dc:creator>Michel André</dc:creator><description>Automated classification of animal sounds is a prerequisite for large-scale
monitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the
most promising algorithms but they are slow, often achieve poor classification
in the field and typically require large training data sets. Our objective was
to design CNNs that are fast at inference time and achieve good classification
performance while learning from moderate-sized data. Recordings from a
rainforest ecosystem were used. Start and end-point of sounds from 20 bird
species were manually annotated. Spectrograms from 10 second segments were used
as CNN input. We designed simple CNNs with a frequency unwrapping layer
(SIMP-FU models) such that any output unit was connected to all spectrogram
frequencies but only to a sub-region of time, the Receptive Field (RF). Our
models allowed experimentation with different RF durations. Models either used
the time-indexed labels that encode start and end-point of sounds or simpler
segment-level labels. Models learning from time-indexed labels performed
considerably better than their segment-level counterparts. Best classification
performances was achieved for models with intermediate RF duration of 1.5
seconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on
the test set. On compact low-cost hardware the best SIMP-FU models evaluated up
to seven times faster than real-time data acquisition. RF duration was a major
driver of classification performance. The optimum of 1.5 s was in the same
range as the duration of the sounds. Our models achieved good classification
performance while learning from moderate-sized training data. This is explained
by the usage of time-indexed labels during training and adequately sized RF.
Results confirm the feasibility of deploying small CNNs with good
classification performance on compact low-cost devices.</description><link>https://arxiv.org/abs/2312.03666</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03666</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Class-Discriminative Attention Maps for Vision Transformers</title><dc:creator>Lennart Brocki</dc:creator><dc:creator>Neo Christopher Chung</dc:creator><description>Interpretability methods are critical components for examining and exploring
deep neural networks (DNN), as well as increasing our understanding of and
trust in them. Vision transformers (ViT), which can be trained to
state-of-the-art performance with a self-supervised learning (SSL) training
method, provide built-in attention maps (AM). While AMs can provide
high-quality semantic segmentation of input images, they do not account for any
signal coming from a downstream classifier. We introduce class-discriminative
attention maps (CDAM), a novel post-hoc explanation method that is highly
sensitive to the target class. Our method essentially scales attention scores
by how relevant the corresponding tokens are for the predictions of a
classifier head. Alternative to classifier outputs, CDAM can also explain a
user-defined concept by targeting similarity measures in the latent space of
the ViT. This allows for explanations of arbitrary concepts, defined by the
user through a few sample images. We investigate the operating characteristics
of CDAM in comparison with relevance propagation (RP) and token ablation maps
(TAM), an alternative to pixel occlusion methods. CDAM is highly
class-discriminative and semantically relevant, while providing implicit
regularization of relevance scores.
  PyTorch implementation: \url{https://github.com/lenbrocki/CDAM}
  Web live demo: \url{https://cdam.informatism.com/}</description><link>https://arxiv.org/abs/2312.02364</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02364</guid><pubDate>Mon, 04 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Rethinking Radiology Report Generation via Causal Reasoning and
  Counterfactual Augmentation</title><dc:creator>Xiao Song</dc:creator><dc:creator>Jiafan Liu</dc:creator><dc:creator>Yun Li</dc:creator><dc:creator>Wenbin Lei</dc:creator><dc:creator>Ruxin Wang</dc:creator><description>Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of
disease co-occurrence as a confounder that effects the results through backdoor
path. Unfortunately, this confounder confuses the process of report generation
worse because of the biased RRG data distribution. In this paper, to rethink
this issue thoroughly, we reason about its causes and effects from a novel
perspective of statistics and causality, where the Joint Vision Coupling and
the Conditional Sentence Coherence Coupling are two aspects prone to implicitly
decrease the accuracy of reports. Then, a counterfactual augmentation strategy
that contains the Counterfactual Sample Synthesis and the Counterfactual Report
Reconstruction sub-methods is proposed to break these two aspects of spurious
effects. Experimental results and further analyses on two widely used datasets
justify our reasoning and proposed methods.</description><link>https://arxiv.org/abs/2311.13307</link><guid isPermaLink="true">https://arxiv.org/abs/2311.13307</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Diffused Task-Agnostic Milestone Planner</title><dc:creator>Mineui Hong</dc:creator><dc:creator>Minjae Kang</dc:creator><dc:creator>Songhwai Oh</dc:creator><description>Addressing decision-making problems using sequence modeling to predict future
trajectories shows promising results in recent years. In this paper, we take a
step further to leverage the sequence predictive method in wider areas such as
long-term planning, vision-based control, and multi-task decision-making. To
this end, we propose a method to utilize a diffusion-based generative sequence
model to plan a series of milestones in a latent space and to have an agent to
follow the milestones to accomplish a given task. The proposed method can learn
control-relevant, low-dimensional latent representations of milestones, which
makes it possible to efficiently perform long-term planning and vision-based
control. Furthermore, our approach exploits generation flexibility of the
diffusion model, which makes it possible to plan diverse trajectories for
multi-task decision-making. We demonstrate the proposed method across offline
reinforcement learning (RL) benchmarks and an visual manipulation environment.
The results show that our approach outperforms offline RL methods in solving
long-horizon, sparse-reward tasks and multi-task problems, while also achieving
the state-of-the-art performance on the most challenging vision-based
manipulation benchmark.</description><link>https://arxiv.org/abs/2312.03395</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03395</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context
  Learning</title><dc:creator>Bill Yuchen Lin</dc:creator><dc:creator>Abhilasha Ravichander</dc:creator><dc:creator>Ximing Lu</dc:creator><dc:creator>Nouha Dziri</dc:creator><dc:creator>Melanie Sclar</dc:creator><dc:creator>Khyathi Chandu</dc:creator><dc:creator>Chandra Bhagavatula</dc:creator><dc:creator>Yejin Choi</dc:creator><description>The alignment tuning process of large language models (LLMs) typically
involves instruction learning through supervised fine-tuning (SFT) and
preference tuning via reinforcement learning from human feedback (RLHF). A
recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for
SFT can achieve significant alignment performance as well, suggesting that the
effect of alignment tuning might be "superficial." This raises questions about
how exactly the alignment tuning transforms a base LLM.
  We analyze the effect of alignment tuning by examining the token distribution
shift between base LLMs and their aligned counterpart. Our findings reveal that
base LLMs and their alignment-tuned versions perform nearly identically in
decoding on the majority of token positions. Most distribution shifts occur
with stylistic tokens. These direct evidence strongly supports the Superficial
Alignment Hypothesis suggested by LIMA.
  Based on these findings, we rethink the alignment of LLMs by posing the
research question: how effectively can we align base LLMs without SFT or RLHF?
To address this, we introduce a simple, tuning-free alignment method, URIAL.
URIAL achieves effective alignment purely through in-context learning (ICL)
with base LLMs, requiring as few as three constant stylistic examples and a
system prompt. We conduct a fine-grained and interpretable evaluation on a
diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that
base LLMs with URIAL can match or even surpass the performance of LLMs aligned
with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based
alignment methods can be significantly reduced through strategic prompting and
ICL. Our findings on the superficial nature of alignment tuning and results
with URIAL suggest that deeper analysis and theoretical understanding of
alignment is crucial to future LLM research.</description><link>https://arxiv.org/abs/2312.01552</link><guid isPermaLink="true">https://arxiv.org/abs/2312.01552</guid><pubDate>Mon, 04 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Rethinking Label Smoothing on Multi-hop Question Answering</title><dc:creator>Zhangyue Yin</dc:creator><dc:creator>Yuxin Wang</dc:creator><dc:creator>Xiannian Hu</dc:creator><dc:creator>Yiguang Wu</dc:creator><dc:creator>Hang Yan</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Zhao Cao</dc:creator><dc:creator>Xuanjing Huang</dc:creator><dc:creator>Xipeng Qiu</dc:creator><description>Multi-Hop Question Answering (MHQA) is a significant area in question
answering, requiring multiple reasoning components, including document
retrieval, supporting sentence prediction, and answer span extraction. In this
work, we analyze the primary factors limiting the performance of multi-hop
reasoning and introduce label smoothing into the MHQA task. This is aimed at
enhancing the generalization capabilities of MHQA systems and mitigating
overfitting of answer spans and reasoning paths in training set. We propose a
novel label smoothing technique, F1 Smoothing, which incorporates uncertainty
into the learning process and is specifically tailored for Machine Reading
Comprehension (MRC) tasks. Inspired by the principles of curriculum learning,
we introduce the Linear Decay Label Smoothing Algorithm (LDLA), which
progressively reduces uncertainty throughout the training process. Experiment
on the HotpotQA dataset demonstrates the effectiveness of our methods in
enhancing performance and generalizability in multi-hop reasoning, achieving
new state-of-the-art results on the leaderboard.</description><link>https://arxiv.org/abs/2212.09512</link><guid isPermaLink="true">https://arxiv.org/abs/2212.09512</guid><pubDate>Mon, 04 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Holmes: Towards Distributed Training Across Clusters with Heterogeneous
  NIC Environment</title><dc:creator>Fei Yang</dc:creator><dc:creator>Shuang Peng</dc:creator><dc:creator>Ning Sun</dc:creator><dc:creator>Fangyu Wang</dc:creator><dc:creator>Ke Tan</dc:creator><dc:creator>Fu Wu</dc:creator><dc:creator>Jiezhong Qiu</dc:creator><dc:creator>Aimin Pan</dc:creator><description>Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated
remarkable accuracy in a wide range of tasks. However, training these models
can incur significant expenses, often requiring tens of thousands of GPUs for
months of continuous operation. Typically, this training is carried out in
specialized GPU clusters equipped with homogeneous high-speed Remote Direct
Memory Access (RDMA) network interface cards (NICs). The acquisition and
maintenance of such dedicated clusters is challenging. Current LLM training
frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on
optimizing training within homogeneous cluster settings. In this paper, we
introduce Holmes, a training framework for LLMs that employs thoughtfully
crafted data and model parallelism strategies over the heterogeneous NIC
environment. Our primary technical contribution lies in a novel scheduling
method that intelligently allocates distinct computational tasklets in LLM
training to specific groups of GPU devices based on the characteristics of
their connected NICs. Furthermore, our proposed framework, utilizing pipeline
parallel techniques, demonstrates scalability to multiple GPU clusters, even in
scenarios without high-speed interconnects between nodes in distinct clusters.
We conducted comprehensive experiments that involved various scenarios in the
heterogeneous NIC environment. In most cases, our framework achieves
performance levels close to those achievable with homogeneous RDMA-capable
networks (InfiniBand or RoCE), significantly exceeding training efficiency
within the pure Ethernet environment. Additionally, we verified that our
framework outperforms other mainstream LLM frameworks under heterogeneous NIC
environment in terms of training efficiency and can be seamlessly integrated
with them.</description><link>https://arxiv.org/abs/2312.03549</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03549</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>A Transformer Model for Symbolic Regression towards Scientific Discovery</title><dc:creator>Florian Lalande</dc:creator><dc:creator>Yoshitomo Matsubara</dc:creator><dc:creator>Naoya Chiba</dc:creator><dc:creator>Tatsunori Taniai</dc:creator><dc:creator>Ryo Igarashi</dc:creator><dc:creator>Yoshitala Ushiku</dc:creator><description>Symbolic Regression (SR) searches for mathematical expressions which best
describe numerical datasets. This allows to circumvent interpretation issues
inherent to artificial neural networks, but SR algorithms are often
computationally expensive. This work proposes a new Transformer model aiming at
Symbolic Regression particularly focused on its application for Scientific
Discovery. We propose three encoder architectures with increasing flexibility
but at the cost of column-permutation equivariance violation. Training results
indicate that the most flexible architecture is required to prevent from
overfitting. Once trained, we apply our best model to the SRSD datasets
(Symbolic Regression for Scientific Discovery datasets) which yields
state-of-the-art results using the normalized tree-based edit distance, at no
extra computational cost.</description><link>https://arxiv.org/abs/2312.04070</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04070</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Causal Estimation of Exposure Shifts with Neural Networks: Evaluating
  the Health Benefits of Stricter Air Quality Standards in the US</title><dc:creator>Mauricio Tec</dc:creator><dc:creator>Oladimeji Mudele</dc:creator><dc:creator>Kevin Josey</dc:creator><dc:creator>Francesca Dominici</dc:creator><description>In policy research, one of the most critical analytic tasks is to estimate
the causal effect of a policy-relevant shift to the distribution of a
continuous exposure/treatment on an outcome of interest. We call this problem
shift-response function (SRF) estimation. Existing neural network methods
involving robust causal-effect estimators lack theoretical guarantees and
practical implementations for SRF estimation. Motivated by a key
policy-relevant question in public health, we develop a neural network method
and its theoretical underpinnings to estimate SRFs with robustness and
efficiency guarantees. We then apply our method to data consisting of 68
million individuals and 27 million deaths across the U.S. to estimate the
causal effect from revising the US National Ambient Air Quality Standards
(NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been
recently proposed by the US Environmental Protection Agency (EPA). Our goal is
to estimate, for the first time, the reduction in deaths that would result from
this anticipated revision using causal methods for SRFs. Our proposed method,
called {T}argeted {R}egularization for {E}xposure {S}hifts with Neural
{Net}works (TRESNET), contributes to the neural network literature for causal
inference in two ways: first, it proposes a targeted regularization loss with
theoretical properties that ensure double robustness and achieves asymptotic
efficiency specific for SRF estimation; second, it enables loss functions from
the exponential family of distributions to accommodate non-continuous outcome
distributions (such as hospitalization or mortality counts). We complement our
application with benchmark experiments that demonstrate TRESNET's broad
applicability and competitiveness.</description><link>https://arxiv.org/abs/2302.02560</link><guid isPermaLink="true">https://arxiv.org/abs/2302.02560</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>When it Rains, it Pours: Modeling Media Storms and the News Ecosystem</title><dc:creator>Benjamin Litterer</dc:creator><dc:creator>David Jurgens</dc:creator><dc:creator>Dallas Card</dc:creator><description>Most events in the world receive at most brief coverage by the news media.
Occasionally, however, an event will trigger a media storm, with voluminous and
widespread coverage lasting for weeks instead of days. In this work, we develop
and apply a pairwise article similarity model, allowing us to identify story
clusters in corpora covering local and national online news, and thereby create
a comprehensive corpus of media storms over a nearly two year period. Using
this corpus, we investigate media storms at a new level of granularity,
allowing us to validate claims about storm evolution and topical distribution,
and provide empirical support for previously hypothesized patterns of influence
of storms on media coverage and intermedia agenda setting.</description><link>https://arxiv.org/abs/2312.02118</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02118</guid><pubDate>Mon, 04 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Unsupervised Keypoints from Pretrained Diffusion Models</title><dc:creator>Eric Hedlin</dc:creator><dc:creator>Gopal Sharma</dc:creator><dc:creator>Shweta Mahajan</dc:creator><dc:creator>Xingzhe He</dc:creator><dc:creator>Hossam Isack</dc:creator><dc:creator>Abhishek Kar Helge Rhodin</dc:creator><dc:creator>Andrea Tagliasacchi</dc:creator><dc:creator>Kwang Moo Yi</dc:creator><description>Unsupervised learning of keypoints and landmarks has seen significant
progress with the help of modern neural network architectures, but performance
is yet to match the supervised counterpart, making their practicability
questionable. We leverage the emergent knowledge within text-to-image diffusion
models, towards more robust unsupervised keypoints. Our core idea is to find
text embeddings that would cause the generative model to consistently attend to
compact regions in images (i.e. keypoints). To do so, we simply optimize the
text embedding such that the cross-attention maps within the denoising network
are localized as Gaussians with small standard deviations. We validate our
performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,
DeepFashion, and Human3.6m datasets. We achieve significantly improved
accuracy, sometimes even outperforming supervised ones, particularly for data
that is non-aligned and less curated. Our code is publicly available and can be
found through our project page: https://ubc-vision.github.io/StableKeypoints/</description><link>https://arxiv.org/abs/2312.00065</link><guid isPermaLink="true">https://arxiv.org/abs/2312.00065</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Decoding Data Quality via Synthetic Corruptions: Embedding-guided
  Pruning of Code Data</title><dc:creator>Yu Yang</dc:creator><dc:creator>Aaditya K. Singh</dc:creator><dc:creator>Mostafa Elhoushi</dc:creator><dc:creator>Anas Mahmoud</dc:creator><dc:creator>Kushal Tirumala</dc:creator><dc:creator>Fabian Gloeckle</dc:creator><dc:creator>Baptiste Rozière</dc:creator><dc:creator>Carole-Jean Wu</dc:creator><dc:creator>Ari S. Morcos</dc:creator><dc:creator>Newsha Ardalani</dc:creator><description>Code datasets, often collected from diverse and uncontrolled sources such as
GitHub, potentially suffer from quality issues, thereby affecting the
performance and training efficiency of Large Language Models (LLMs) optimized
for code generation. Previous studies demonstrated the benefit of using
embedding spaces for data pruning, but they mainly focused on duplicate removal
or increasing variety, and in other modalities, such as images. Our work
focuses on using embeddings to identify and remove "low-quality" code data.
First, we explore features of "low-quality" code in embedding space, through
the use of synthetic corruptions. Armed with this knowledge, we devise novel
pruning metrics that operate in embedding space to identify and remove
low-quality entries in the Stack dataset. We demonstrate the benefits of this
synthetic corruption informed pruning (SCIP) approach on the well-established
HumanEval and MBPP benchmarks, outperforming existing embedding-based methods.
Importantly, we achieve up to a 3% performance improvement over no pruning,
thereby showing the promise of insights from synthetic corruptions for data
pruning.</description><link>https://arxiv.org/abs/2312.02418</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02418</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Seller-side Outcome Fairness in Online Marketplaces</title><dc:creator>Zikun Ye</dc:creator><dc:creator>Reza Yousefi Maragheh</dc:creator><dc:creator>Lalitesh Morishetti</dc:creator><dc:creator>Shanu Vashishtha</dc:creator><dc:creator>Jason Cho</dc:creator><dc:creator>Kaushiki Nag</dc:creator><dc:creator>Sushant Kumar</dc:creator><dc:creator>Kannan Achan</dc:creator><description>This paper aims to investigate and achieve seller-side fairness within online
marketplaces, where many sellers and their items are not sufficiently exposed
to customers in an e-commerce platform. This phenomenon raises concerns
regarding the potential loss of revenue associated with less exposed items as
well as less marketplace diversity. We introduce the notion of seller-side
outcome fairness and build an optimization model to balance collected
recommendation rewards and the fairness metric. We then propose a
gradient-based data-driven algorithm based on the duality and bandit theory.
Our numerical experiments on real e-commerce data sets show that our algorithm
can lift seller fairness measures while not hurting metrics like collected
Gross Merchandise Value (GMV) and total purchases.</description><link>https://arxiv.org/abs/2312.03253</link><guid isPermaLink="true">https://arxiv.org/abs/2312.03253</guid><pubDate>Wed, 06 Dec 2023 00:00:00 -0000</pubDate></item><item><title>DreaMo: Articulated 3D Reconstruction From A Single Casual Video</title><dc:creator>Tao Tu</dc:creator><dc:creator>Ming-Feng Li</dc:creator><dc:creator>Chieh Hubert Lin</dc:creator><dc:creator>Yen-Chi Cheng</dc:creator><dc:creator>Min Sun</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><description>Articulated 3D reconstruction has valuable applications in various domains,
yet it remains costly and demands intensive work from domain experts. Recent
advancements in template-free learning methods show promising results with
monocular videos. Nevertheless, these approaches necessitate a comprehensive
coverage of all viewpoints of the subject in the input video, thus limiting
their applicability to casually captured videos from online sources. In this
work, we study articulated 3D shape reconstruction from a single and casually
captured internet video, where the subject's view coverage is incomplete. We
propose DreaMo that jointly performs shape reconstruction while solving the
challenging low-coverage regions with view-conditioned diffusion prior and
several tailored regularizations. In addition, we introduce a skeleton
generation strategy to create human-interpretable skeletons from the learned
neural bones and skinning weights. We conduct our study on a self-collected
internet video collection characterized by incomplete view coverage. DreaMo
shows promising quality in novel-view rendering, detailed articulated shape
reconstruction, and skeleton generation. Extensive qualitative and quantitative
studies validate the efficacy of each proposed component, and show existing
methods are unable to solve correct geometry due to the incomplete view
coverage.</description><link>https://arxiv.org/abs/2312.02617</link><guid isPermaLink="true">https://arxiv.org/abs/2312.02617</guid><pubDate>Tue, 05 Dec 2023 00:00:00 -0000</pubDate></item></channel></rss>