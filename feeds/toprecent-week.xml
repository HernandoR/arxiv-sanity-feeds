<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv Sanity Recent Feed (Week)</title><description>Top papers based on people's libraries.</description><link>https://hedgehog.den.dev/feeds/toprecent-week.xml</link><atom:link href="https://hedgehog.den.dev/feeds/toprecent-week.xml" rel="self" type="application/rss+xml" /><item><title>Scaling Laws of Synthetic Images for Model Training ... for Now</title><dc:creator>Lijie Fan</dc:creator><dc:creator>Kaifeng Chen</dc:creator><dc:creator>Dilip Krishnan</dc:creator><dc:creator>Dina Katabi</dc:creator><dc:creator>Phillip Isola</dc:creator><dc:creator>Yonglong Tian</dc:creator><description>Recent significant advances in text-to-image models unlock the possibility of
training vision systems using synthetic images, potentially overcoming the
difficulty of collecting curated data at scale. It is unclear, however, how
these models behave at scale, as more synthetic data is added to the training
set. In this paper we study the scaling laws of synthetic images generated by
state of the art text-to-image models, for the training of supervised models:
image classifiers with label supervision, and CLIP with language supervision.
We identify several factors, including text prompts, classifier-free guidance
scale, and types of text-to-image models, that significantly affect scaling
behavior. After tuning these factors, we observe that synthetic images
demonstrate a scaling trend similar to, but slightly less effective than, real
images in CLIP training, while they significantly underperform in scaling when
training supervised image classifiers. Our analysis indicates that the main
reason for this underperformance is the inability of off-the-shelf
text-to-image models to generate certain concepts, a limitation that
significantly impairs the training of image classifiers. Our findings also
suggest that scaling synthetic data can be particularly effective in scenarios
such as: (1) when there is a limited supply of real images for a supervised
problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the
evaluation dataset diverges significantly from the training data, indicating
the out-of-distribution scenario, or (3) when synthetic data is used in
conjunction with real images, as demonstrated in the training of CLIP models.</description><link>https://arxiv.org/abs/2312.04567</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04567</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Gen2Det: Generate to Detect</title><dc:creator>Saksham Suri</dc:creator><dc:creator>Fanyi Xiao</dc:creator><dc:creator>Animesh Sinha</dc:creator><dc:creator>Sean Chang Culatana</dc:creator><dc:creator>Raghuraman Krishnamoorthi</dc:creator><dc:creator>Chenchen Zhu</dc:creator><dc:creator>Abhinav Shrivastava</dc:creator><description>Recently diffusion models have shown improvement in synthetic image quality
as well as better control in generation. We motivate and present Gen2Det, a
simple modular pipeline to create synthetic training data for object detection
for free by leveraging state-of-the-art grounded image generation methods.
Unlike existing works which generate individual object instances, require
identifying foreground followed by pasting on other images, we simplify to
directly generating scene-centric images. In addition to the synthetic data,
Gen2Det also proposes a suite of techniques to best utilize the generated data,
including image-level filtering, instance-level filtering, and better training
recipe to account for imperfections in the generation. Using Gen2Det, we show
healthy improvements on object detection and segmentation tasks under various
settings and agnostic to detection methods. In the long-tailed detection
setting on LVIS, Gen2Det improves the performance on rare categories by a large
margin while also significantly improving the performance on other categories,
e.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training
on real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO,
Gen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In
the most general detection setting, Gen2Det still demonstrates robust
performance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and
0.32 points.</description><link>https://arxiv.org/abs/2312.04566</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04566</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>MuRF: Multi-Baseline Radiance Fields</title><dc:creator>Haofei Xu</dc:creator><dc:creator>Anpei Chen</dc:creator><dc:creator>Yuedong Chen</dc:creator><dc:creator>Christos Sakaridis</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Marc Pollefeys</dc:creator><dc:creator>Andreas Geiger</dc:creator><dc:creator>Fisher Yu</dc:creator><description>We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward
approach to solving sparse view synthesis under multiple different baseline
settings (small and large baselines, and different number of input views). To
render a target novel view, we discretize the 3D space into planes parallel to
the target image plane, and accordingly construct a target view frustum volume.
Such a target volume representation is spatially aligned with the target view,
which effectively aggregates relevant information from the input views for
high-quality rendering. It also facilitates subsequent radiance field
regression with a convolutional network thanks to its axis-aligned nature. The
3D context modeled by the convolutional network enables our method to synthesis
sharper scene structures than prior works. Our MuRF achieves state-of-the-art
performance across multiple different baseline settings and diverse scenarios
ranging from simple objects (DTU) to complex indoor and outdoor scenes
(RealEstate10K and LLFF). We also show promising zero-shot generalization
abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability
of MuRF.</description><link>https://arxiv.org/abs/2312.04565</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04565</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS</title><dc:creator>Sharath Girish</dc:creator><dc:creator>Kamal Gupta</dc:creator><dc:creator>Abhinav Shrivastava</dc:creator><description>Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view
scene synthesis. It addresses the challenges of lengthy training times and slow
rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,
differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time
rendering and accelerated training. They, however, demand substantial memory
resources for both training and storage, as they require millions of Gaussians
in their point cloud representation for each scene. We present a technique
utilizing quantized embeddings to significantly reduce memory storage
requirements and a coarse-to-fine training strategy for a faster and more
stable optimization of the Gaussian point clouds. Our approach results in scene
representations with fewer Gaussians and quantized representations, leading to
faster training times and rendering speeds for real-time rendering of high
resolution scenes. We reduce memory by more than an order of magnitude all
while maintaining the reconstruction quality. We validate the effectiveness of
our approach on a variety of datasets and scenes preserving the visual quality
while consuming 10-20x less memory and faster training/inference speed. Project
page and code is available https://efficientgaussian.github.io</description><link>https://arxiv.org/abs/2312.04564</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04564</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Visual Geometry Grounded Deep Structure From Motion</title><dc:creator>Jianyuan Wang</dc:creator><dc:creator>Nikita Karaev</dc:creator><dc:creator>Christian Rupprecht</dc:creator><dc:creator>David Novotny</dc:creator><description>Structure-from-motion (SfM) is a long-standing problem in the computer vision
community, which aims to reconstruct the camera poses and 3D structure of a
scene from a set of unconstrained 2D images. Classical frameworks solve this
problem in an incremental manner by detecting and matching keypoints,
registering images, triangulating 3D points, and conducting bundle adjustment.
Recent research efforts have predominantly revolved around harnessing the power
of deep learning techniques to enhance specific elements (e.g., keypoint
matching), but are still based on the original, non-differentiable pipeline.
Instead, we propose a new deep pipeline VGGSfM, where each component is fully
differentiable and thus can be trained in an end-to-end manner. To this end, we
introduce new mechanisms and simplifications. First, we build on recent
advances in deep 2D point tracking to extract reliable pixel-accurate tracks,
which eliminates the need for chaining pairwise matches. Furthermore, we
recover all cameras simultaneously based on the image and track features
instead of gradually registering cameras. Finally, we optimise the cameras and
triangulate 3D points via a differentiable bundle adjustment layer. We attain
state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,
and ETH3D.</description><link>https://arxiv.org/abs/2312.04563</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04563</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>NeRFiller: Completing Scenes via Generative 3D Inpainting</title><dc:creator>Ethan Weber</dc:creator><dc:creator>Aleksander Hołyński</dc:creator><dc:creator>Varun Jampani</dc:creator><dc:creator>Saurabh Saxena</dc:creator><dc:creator>Noah Snavely</dc:creator><dc:creator>Abhishek Kar</dc:creator><dc:creator>Angjoo Kanazawa</dc:creator><description>We propose NeRFiller, an approach that completes missing portions of a 3D
capture via generative 3D inpainting using off-the-shelf 2D visual generative
models. Often parts of a captured 3D scene or object are missing due to mesh
reconstruction failures or a lack of observations (e.g., contact regions, such
as the bottom of objects, or hard-to-reach areas). We approach this challenging
3D inpainting problem by leveraging a 2D inpainting diffusion model. We
identify a surprising behavior of these models, where they generate more 3D
consistent inpaints when images form a 2$\times$2 grid, and show how to
generalize this behavior to more than four images. We then present an iterative
framework to distill these inpainted regions into a single consistent 3D scene.
In contrast to related works, we focus on completing scenes rather than
deleting foreground objects, and our approach does not require tight 2D object
masks or text. We compare our approach to relevant baselines adapted to our
setting on a variety of scenes, where NeRFiller creates the most 3D consistent
and plausible scene completions. Our project page is at
https://ethanweber.me/nerfiller.</description><link>https://arxiv.org/abs/2312.04560</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04560</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>GenDeF: Learning Generative Deformation Field for Video Generation</title><dc:creator>Wen Wang</dc:creator><dc:creator>Kecheng Zheng</dc:creator><dc:creator>Qiuyu Wang</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Zifan Shi</dc:creator><dc:creator>Ceyuan Yang</dc:creator><dc:creator>Yujun Shen</dc:creator><dc:creator>Chunhua Shen</dc:creator><description>We offer a new perspective on approaching the task of video generation.
Instead of directly synthesizing a sequence of frames, we propose to render a
video by warping one static image with a generative deformation field (GenDeF).
Such a pipeline enjoys three appealing advantages. First, we can sufficiently
reuse a well-trained image generator to synthesize the static image (also
called canonical image), alleviating the difficulty in producing a video and
thereby resulting in better visual quality. Second, we can easily convert a
deformation field to optical flows, making it possible to apply explicit
structural regularizations for motion modeling, leading to temporally
consistent results. Third, the disentanglement between content and motion
allows users to process a synthesized video through processing its
corresponding static image without any tuning, facilitating many applications
like video editing, keypoint tracking, and video segmentation. Both qualitative
and quantitative results on three common video generation benchmarks
demonstrate the superiority of our GenDeF method.</description><link>https://arxiv.org/abs/2312.04561</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04561</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation</title><dc:creator>Zhaoxi Chen</dc:creator><dc:creator>Fangzhou Hong</dc:creator><dc:creator>Haiyi Mei</dc:creator><dc:creator>Guangcong Wang</dc:creator><dc:creator>Lei Yang</dc:creator><dc:creator>Ziwei Liu</dc:creator><description>We present PrimDiffusion, the first diffusion-based framework for 3D human
generation. Devising diffusion models for 3D human generation is difficult due
to the intensive computational cost of 3D representations and the articulated
topology of 3D humans. To tackle these challenges, our key insight is operating
the denoising diffusion process directly on a set of volumetric primitives,
which models the human body as a number of small volumes with radiance and
kinematic information. This volumetric primitives representation marries the
capacity of volumetric representations with the efficiency of primitive-based
rendering. Our PrimDiffusion framework has three appealing properties: 1)
compact and expressive parameter space for the diffusion model, 2) flexible 3D
representation that incorporates human prior, and 3) decoder-free rendering for
efficient novel-view and novel-pose synthesis. Extensive experiments validate
that PrimDiffusion outperforms state-of-the-art methods in 3D human generation.
Notably, compared to GAN-based methods, our PrimDiffusion supports real-time
rendering of high-quality 3D humans at a resolution of $512\times512$ once the
denoising process is done. We also demonstrate the flexibility of our framework
on training-free conditional generation such as texture transfer and 3D
inpainting.</description><link>https://arxiv.org/abs/2312.04559</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04559</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</title><dc:creator>Yufan Chen</dc:creator><dc:creator>Lizhen Wang</dc:creator><dc:creator>Qijing Li</dc:creator><dc:creator>Hongjiang Xiao</dc:creator><dc:creator>Shengping Zhang</dc:creator><dc:creator>Hongxun Yao</dc:creator><dc:creator>Yebin Liu</dc:creator><description>The ability to animate photo-realistic head avatars reconstructed from
monocular portrait video sequences represents a crucial step in bridging the
gap between the virtual and real worlds. Recent advancements in head avatar
techniques, including explicit 3D morphable meshes (3DMM), point clouds, and
neural implicit representation have been exploited for this ongoing research.
However, 3DMM-based methods are constrained by their fixed topologies,
point-based approaches suffer from a heavy training burden due to the extensive
quantity of points involved, and the last ones suffer from limitations in
deformation flexibility and rendering efficiency. In response to these
challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head
Avatar), a novel approach that harnesses 3D Gaussian point representation
coupled with a Gaussian deformation field to learn explicit head avatars from
monocular portrait videos. We define our head avatars with Gaussian points
characterized by adaptable shapes, enabling flexible topology. These points
exhibit movement with a Gaussian deformation field in alignment with the target
pose and expression of a person, facilitating efficient deformation.
Additionally, the Gaussian points have controllable shape, size, color, and
opacity combined with Gaussian splatting, allowing for efficient training and
rendering. Experiments demonstrate the superior performance of our method,
which achieves state-of-the-art results among previous methods.</description><link>https://arxiv.org/abs/2312.04558</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04558</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>GenTron: Delving Deep into Diffusion Transformers for Image and Video
  Generation</title><dc:creator>Shoufa Chen</dc:creator><dc:creator>Mengmeng Xu</dc:creator><dc:creator>Jiawei Ren</dc:creator><dc:creator>Yuren Cong</dc:creator><dc:creator>Sen He</dc:creator><dc:creator>Yanping Xie</dc:creator><dc:creator>Animesh Sinha</dc:creator><dc:creator>Ping Luo</dc:creator><dc:creator>Tao Xiang</dc:creator><dc:creator>Juan-Manuel Perez-Rua</dc:creator><description>In this study, we explore Transformer-based diffusion models for image and
video generation. Despite the dominance of Transformer architectures in various
fields due to their flexibility and scalability, the visual generative domain
primarily utilizes CNN-based U-Net architectures, particularly in
diffusion-based models. We introduce GenTron, a family of Generative models
employing Transformer-based diffusion, to address this gap. Our initial step
was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a
process involving thorough empirical exploration of the conditioning mechanism.
We then scale GenTron from approximately 900M to over 3B parameters, observing
significant improvements in visual quality. Furthermore, we extend GenTron to
text-to-video generation, incorporating novel motion-free guidance to enhance
video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win
rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text
alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,
underscoring its strengths in compositional generation. We believe this work
will provide meaningful insights and serve as a valuable reference for future
research.</description><link>https://arxiv.org/abs/2312.04557</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04557</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Large Language Models for Mathematicians</title><dc:creator>Simon Frieder</dc:creator><dc:creator>Julius Berner</dc:creator><dc:creator>Philipp Petersen</dc:creator><dc:creator>Thomas Lukasiewicz</dc:creator><description>Large language models (LLMs) such as ChatGPT have received immense interest
for their general-purpose language understanding and, in particular, their
ability to generate high-quality text or computer code. For many professions,
LLMs represent an invaluable tool that can speed up and improve the quality of
work. In this note, we discuss to what extent they can aid professional
mathematicians. We first provide a mathematical description of the transformer
model used in all modern language models. Based on recent studies, we then
outline best practices and potential issues and report on the mathematical
abilities of language models. Finally, we shed light on the potential of LMMs
to change how mathematicians work.</description><link>https://arxiv.org/abs/2312.04556</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04556</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Improved Visual Grounding through Self-Consistent Explanations</title><dc:creator>Ruozhen He</dc:creator><dc:creator>Paola Cascante-Bonilla</dc:creator><dc:creator>Ziyan Yang</dc:creator><dc:creator>Alexander C. Berg</dc:creator><dc:creator>Vicente Ordonez</dc:creator><description>Vision-and-language models trained to match images with text can be combined
with visual explanation methods to point to the locations of specific objects
in an image. Our work shows that the localization --"grounding"-- abilities of
these models can be further improved by finetuning for self-consistent visual
explanations. We propose a strategy for augmenting existing text-image datasets
with paraphrases using a large language model, and SelfEQ, a weakly-supervised
strategy on visual explanation maps for paraphrases that encourages
self-consistency. Specifically, for an input textual phrase, we attempt to
generate a paraphrase and finetune the model so that the phrase and paraphrase
map to the same region in the image. We posit that this both expands the
vocabulary that the model is able to handle, and improves the quality of the
object locations highlighted by gradient-based visual explanation methods (e.g.
GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,
ReferIt, and RefCOCO+ over a strong baseline method and several prior works.
Particularly, comparing to other methods that do not use any type of box
annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),
67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on
RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on
average).</description><link>https://arxiv.org/abs/2312.04554</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04554</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>SPIDeRS: Structured Polarization for Invisible Depth and Reflectance
  Sensing</title><dc:creator>Tomoki Ichikawa</dc:creator><dc:creator>Shohei Nobuhara</dc:creator><dc:creator>Ko Nishino</dc:creator><description>Can we capture shape and reflectance in stealth? Such capability would be
valuable for many application domains in vision, xR, robotics, and HCI. We
introduce Structured Polarization, the first depth and reflectance sensing
method using patterns of polarized light (SPIDeRS). The key idea is to modulate
the angle of linear polarization (AoLP) of projected light at each pixel. The
use of polarization makes it invisible and lets us recover not only depth but
also directly surface normals and even reflectance. We implement SPIDeRS with a
liquid crystal spatial light modulator (SLM) and a polarimetric camera. We
derive a novel method for robustly extracting the projected structured
polarization pattern from the polarimetric object appearance. We evaluate the
effectiveness of SPIDeRS by applying it to a number of real-world objects. The
results show that our method successfully reconstructs object shapes of various
materials and is robust to diffuse reflection and ambient light. We also
demonstrate relighting using recovered surface normals and reflectance. We
believe SPIDeRS opens a new avenue of polarization use in visual sensing.</description><link>https://arxiv.org/abs/2312.04553</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04553</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Generating Illustrated Instructions</title><dc:creator>Sachit Menon</dc:creator><dc:creator>Ishan Misra</dc:creator><dc:creator>Rohit Girdhar</dc:creator><description>We introduce the new task of generating Illustrated Instructions, i.e.,
visual instructions customized to a user's needs. We identify desiderata unique
to this task, and formalize it through a suite of automatic and human
evaluation metrics, designed to measure the validity, consistency, and efficacy
of the generations. We combine the power of large language models (LLMs)
together with strong text-to-image generation diffusion models to propose a
simple approach called StackedDiffusion, which generates such illustrated
instructions given text as input. The resulting model strongly outperforms
baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,
users even prefer it to human-generated articles. Most notably, it enables
various new and exciting applications far beyond what static articles on the
web can provide, such as personalized instructions complete with intermediate
steps and pictures in response to a user's individual situation.</description><link>https://arxiv.org/abs/2312.04552</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04552</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Free3D: Consistent Novel View Synthesis without 3D Representation</title><dc:creator>Chuanxia Zheng</dc:creator><dc:creator>Andrea Vedaldi</dc:creator><description>We introduce Free3D, a simple approach designed for open-set novel view
synthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from a
pre-trained 2D image generator for generalization, and fine-tune it for NVS.
Compared to recent and concurrent works, we obtain significant improvements
without resorting to an explicit 3D representation, which is slow and
memory-consuming or training an additional 3D network. We do so by encoding
better the target camera pose via a new per-pixel ray conditioning
normalization (RCN) layer. The latter injects pose information in the
underlying 2D image generator by telling each pixel its specific viewing
direction. We also improve multi-view consistency via a light-weight multi-view
attention layer and multi-view noise sharing. We train Free3D on the Objaverse
dataset and demonstrate excellent generalization to various new categories in
several new datasets, including OminiObject3D and GSO. We hope our simple and
effective approach will serve as a solid baseline and help future research in
NVS with more accuracy pose. The project page is available at
https://chuanxiaz.com/free3d/.</description><link>https://arxiv.org/abs/2312.04551</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04551</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve
  Aerial Visual Perception?</title><dc:creator>Aritra Dutta</dc:creator><dc:creator>Srijan Das</dc:creator><dc:creator>Jacob Nielsen</dc:creator><dc:creator>Rajatsubhra Chakraborty</dc:creator><dc:creator>Mubarak Shah</dc:creator><description>Despite the commercial abundance of UAVs, aerial data acquisition remains
challenging, and the existing Asia and North America-centric open-source UAV
datasets are small-scale or low-resolution and lack diversity in scene
contextuality. Additionally, the color content of the scenes, solar-zenith
angle, and population density of different geographies influence the data
diversity. These two factors conjointly render suboptimal aerial-visual
perception of the deep neural network (DNN) models trained primarily on the
ground-view data, including the open-world foundational models.
  To pave the way for a transformative era of aerial detection, we present
Multiview Aerial Visual RECognition or MAVREC, a video dataset where we record
synchronized scenes from different perspectives -- ground camera and
drone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard
2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million
annotated bounding boxes. This makes MAVREC the largest ground and aerial-view
dataset, and the fourth largest among all drone-based datasets across all
modalities and tasks. Through our extensive benchmarking on MAVREC, we
recognize that augmenting object detectors with ground-view images from the
corresponding geographical location is a superior pre-training strategy for
aerial detection. Building on this strategy, we benchmark MAVREC with a
curriculum-based semi-supervised object detection approach that leverages
labeled (ground and aerial) and unlabeled (only aerial) images to enhance the
aerial detection. We publicly release the MAVREC dataset:
https://mavrec.github.io.</description><link>https://arxiv.org/abs/2312.04548</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04548</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play</title><dc:creator>Lili Chen</dc:creator><dc:creator>Shikhar Bahl</dc:creator><dc:creator>Deepak Pathak</dc:creator><description>Learning from unstructured and uncurated data has become the dominant
paradigm for generative approaches in language and vision. Such unstructured
and unguided behavior data, commonly known as play, is also easier to collect
in robotics but much more difficult to learn from due to its inherently
multimodal, noisy, and suboptimal nature. In this paper, we study this problem
of learning goal-directed skill policies from unstructured play data which is
labeled with language in hindsight. Specifically, we leverage advances in
diffusion models to learn a multi-task diffusion model to extract robotic
skills from play data. Using a conditional denoising diffusion process in the
space of states and actions, we can gracefully handle the complexity and
multimodality of play data and generate diverse and interesting robot
behaviors. To make diffusion models more useful for skill learning, we
encourage robotic agents to acquire a vocabulary of skills by introducing
discrete bottlenecks into the conditional behavior generation process. In our
experiments, we demonstrate the effectiveness of our approach across a wide
variety of environments in both simulation and the real world. Results
visualizations and videos at https://play-fusion.github.io</description><link>https://arxiv.org/abs/2312.04549</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04549</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Digital Life Project: Autonomous 3D Characters with Social Intelligence</title><dc:creator>Zhongang Cai</dc:creator><dc:creator>Jianping Jiang</dc:creator><dc:creator>Zhongfei Qing</dc:creator><dc:creator>Xinying Guo</dc:creator><dc:creator>Mingyuan Zhang</dc:creator><dc:creator>Zhengyu Lin</dc:creator><dc:creator>Haiyi Mei</dc:creator><dc:creator>Chen Wei</dc:creator><dc:creator>Ruisi Wang</dc:creator><dc:creator>Wanqi Yin</dc:creator><dc:creator>Xiangyu Fan</dc:creator><dc:creator>Han Du</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Peng Gao</dc:creator><dc:creator>Zhitao Yang</dc:creator><dc:creator>Yang Gao</dc:creator><dc:creator>Jiaqi Li</dc:creator><dc:creator>Tianxiang Ren</dc:creator><dc:creator>Yukun Wei</dc:creator><dc:creator>Xiaogang Wang</dc:creator><dc:creator>Chen Change Loy</dc:creator><dc:creator>Lei Yang</dc:creator><dc:creator>Ziwei Liu</dc:creator><description>In this work, we present Digital Life Project, a framework utilizing language
as the universal medium to build autonomous 3D characters, who are capable of
engaging in social interactions and expressing with articulated body motions,
thereby simulating life in a digital environment. Our framework comprises two
primary components: 1) SocioMind: a meticulously crafted digital brain that
models personalities with systematic few-shot exemplars, incorporates a
reflection process based on psychology principles, and emulates autonomy by
initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis
paradigm for controlling the character's digital body. It integrates motion
matching, a proven industry technique to ensure motion quality, with
cutting-edge advancements in motion generation for diversity. Extensive
experiments demonstrate that each module achieves state-of-the-art performance
in its respective domain. Collectively, they enable virtual characters to
initiate and sustain dialogues autonomously, while evolving their
socio-psychological states. Concurrently, these characters can perform
contextually relevant bodily movements. Additionally, a motion captioning
module further allows the virtual character to recognize and appropriately
respond to human players' actions. Homepage: https://digital-life-project.com/</description><link>https://arxiv.org/abs/2312.04547</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04547</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Adversarial Learning for Feature Shift Detection and Correction</title><dc:creator>Miriam Barrabes</dc:creator><dc:creator>Daniel Mas Montserrat</dc:creator><dc:creator>Margarita Geleta</dc:creator><dc:creator>Xavier Giro-i-Nieto</dc:creator><dc:creator>Alexander G. Ioannidis</dc:creator><description>Data shift is a phenomenon present in many real-world applications, and while
there are multiple methods attempting to detect shifts, the task of localizing
and correcting the features originating such shifts has not been studied in
depth. Feature shifts can occur in many datasets, including in multi-sensor
data, where some sensors are malfunctioning, or in tabular and structured data,
including biomedical, financial, and survey data, where faulty standardization
and data processing pipelines can lead to erroneous features. In this work, we
explore using the principles of adversarial learning, where the information
from several discriminators trained to distinguish between two distributions is
used to both detect the corrupted features and fix them in order to remove the
distribution shift between datasets. We show that mainstream supervised
classifiers, such as random forest or gradient boosting trees, combined with
simple iterative heuristics, can localize and correct feature shifts,
outperforming current statistical and neural network-based techniques. The code
is available at https://github.com/AI-sandbox/DataFix.</description><link>https://arxiv.org/abs/2312.04546</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04546</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</title><dc:creator>Zhaoxi Chen</dc:creator><dc:creator>Guangcong Wang</dc:creator><dc:creator>Ziwei Liu</dc:creator><description>In this work, we present SceneDreamer, an unconditional generative model for
unbounded 3D scenes, which synthesizes large-scale 3D landscapes from random
noise. Our framework is learned from in-the-wild 2D image collections only,
without any 3D annotations. At the core of SceneDreamer is a principled
learning paradigm comprising 1) an efficient yet expressive 3D scene
representation, 2) a generative scene parameterization, and 3) an effective
renderer that can leverage the knowledge from 2D images. Our approach begins
with an efficient bird's-eye-view (BEV) representation generated from simplex
noise, which includes a height field for surface elevation and a semantic field
for detailed scene semantics. This BEV scene representation enables 1)
representing a 3D scene with quadratic complexity, 2) disentangled geometry and
semantics, and 3) efficient training. Moreover, we propose a novel generative
neural hash grid to parameterize the latent space based on 3D positions and
scene semantics, aiming to encode generalizable features across various scenes.
Lastly, a neural volumetric renderer, learned from 2D image collections through
adversarial training, is employed to produce photorealistic images. Extensive
experiments demonstrate the effectiveness of SceneDreamer and superiority over
state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.</description><link>https://arxiv.org/abs/2302.01330</link><guid isPermaLink="true">https://arxiv.org/abs/2302.01330</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a
  Single Image</title><dc:creator>Tong Wu</dc:creator><dc:creator>Zhibing Li</dc:creator><dc:creator>Shuai Yang</dc:creator><dc:creator>Pan Zhang</dc:creator><dc:creator>Xinggang Pan</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Ziwei Liu</dc:creator><description>3D content creation from a single image is a long-standing yet highly
desirable task. Recent advances introduce 2D diffusion priors, yielding
reasonable results. However, existing methods are not hyper-realistic enough
for post-generation usage, as users cannot view, render and edit the resulting
3D content from a full range. To address these challenges, we introduce
HyperDreamer with several key designs and appealing properties: 1) Viewable:
360 degree mesh modeling with high-resolution textures enables the creation of
visually compelling 3D models from a full range of observation points. 2)
Renderable: Fine-grained semantic segmentation and data-driven priors are
incorporated as guidance to learn reasonable albedo, roughness, and specular
properties of the materials, enabling semantic-aware arbitrary material
estimation. 3) Editable: For a generated model or their own data, users can
interactively select any region via a few clicks and efficiently edit the
texture with text-based guidance. Extensive experiments demonstrate the
effectiveness of HyperDreamer in modeling region-aware materials with
high-resolution textures and enabling user-friendly editing. We believe that
HyperDreamer holds promise for advancing 3D content creation and finding
applications in various domains.</description><link>https://arxiv.org/abs/2312.04543</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04543</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>SoK: Unintended Interactions among Machine Learning Defenses and Risks</title><dc:creator>Vasisht Duddu</dc:creator><dc:creator>Sebastian Szyller</dc:creator><dc:creator>N. Asokan</dc:creator><description>Machine learning (ML) models cannot neglect risks to security, privacy, and
fairness. Several defenses have been proposed to mitigate such risks. When a
defense is effective in mitigating one risk, it may correspond to increased or
decreased susceptibility to other risks. Existing research lacks an effective
framework to recognize and explain these unintended interactions. We present
such a framework, based on the conjecture that overfitting and memorization
underlie unintended interactions. We survey existing literature on unintended
interactions, accommodating them within our framework. We use our framework to
conjecture on two previously unexplored interactions, and empirically validate
our conjectures.</description><link>https://arxiv.org/abs/2312.04542</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04542</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Sim-to-Real Causal Transfer: A Metric Learning Approach to
  Causally-Aware Interaction Representations</title><dc:creator>Yuejiang Liu</dc:creator><dc:creator>Ahmad Rahimi</dc:creator><dc:creator>Po-Chien Luan</dc:creator><dc:creator>Frano Rajič</dc:creator><dc:creator>Alexandre Alahi</dc:creator><description>Modeling spatial-temporal interactions among neighboring agents is at the
heart of multi-agent problems such as motion forecasting and crowd navigation.
Despite notable progress, it remains unclear to which extent modern
representations can capture the causal relationships behind agent interactions.
In this work, we take an in-depth look at the causal awareness of these
representations, from computational formalism to real-world practice. First, we
cast doubt on the notion of non-causal robustness studied in the recent
CausalAgents benchmark. We show that recent representations are already
partially resilient to perturbations of non-causal agents, and yet modeling
indirect causal effects involving mediator agents remains challenging. To
address this challenge, we introduce a metric learning approach that
regularizes latent representations with causal annotations. Our controlled
experiments show that this approach not only leads to higher degrees of causal
awareness but also yields stronger out-of-distribution robustness. To further
operationalize it in practice, we propose a sim-to-real causal transfer method
via cross-domain multi-task learning. Experiments on pedestrian datasets show
that our method can substantially boost generalization, even in the absence of
real-world causal annotations. We hope our work provides a new perspective on
the challenges and potential pathways towards causally-aware representations of
multi-agent interactions. Our code is available at
https://github.com/socialcausality.</description><link>https://arxiv.org/abs/2312.04540</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04540</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Self-Guided Open-Vocabulary Semantic Segmentation</title><dc:creator>Osman Ülger</dc:creator><dc:creator>Maksymilian Kulicki</dc:creator><dc:creator>Yuki Asano</dc:creator><dc:creator>Martin R. Oswald</dc:creator><description>Vision-Language Models (VLMs) have emerged as promising tools for open-ended
image understanding tasks, including open vocabulary segmentation. Yet, direct
application of such VLMs to segmentation is non-trivial, since VLMs are trained
with image-text pairs and naturally lack pixel-level granularity. Recent works
have made advancements in bridging this gap, often by leveraging the shared
image-text space in which the image and a provided text prompt are represented.
In this paper, we challenge the capabilities of VLMs further and tackle
open-vocabulary segmentation without the need for any textual input. To this
end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework.
Self-Seg is capable of automatically detecting relevant class names from
clustered BLIP embeddings and using these for accurate semantic segmentation.
In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to
effectively assess predicted open-vocabulary class names. We achieve
state-of-the-art results on Pascal VOC, ADE20K and CityScapes for
open-vocabulary segmentation without given class names, as well as competitive
performance with methods where class names are given. All code and data will be
released.</description><link>https://arxiv.org/abs/2312.04539</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04539</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item><item><title>Trajeglish: Learning the Language of Driving Scenarios</title><dc:creator>Jonah Philion</dc:creator><dc:creator>Xue Bin Peng</dc:creator><dc:creator>Sanja Fidler</dc:creator><description>A longstanding challenge for self-driving development is simulating dynamic
driving scenarios seeded from recorded driving logs. In pursuit of this
functionality, we apply tools from discrete sequence modeling to model how
vehicles, pedestrians and cyclists interact in driving scenarios. Using a
simple data-driven tokenization scheme, we discretize trajectories to
centimeter-level resolution using a small vocabulary. We then model the
multi-agent sequence of motion tokens with a GPT-like encoder-decoder that is
autoregressive in time and takes into account intra-timestep interaction
between agents. Scenarios sampled from our model exhibit state-of-the-art
realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work
along the realism meta metric by 3.3% and along the interaction metric by 9.9%.
We ablate our modeling choices in full autonomy and partial autonomy settings,
and show that the representations learned by our model can quickly be adapted
to improve performance on nuScenes. We additionally evaluate the scalability of
our model with respect to parameter count and dataset size, and use density
estimates from our model to quantify the saliency of context length and
intra-timestep interaction for the traffic modeling task.</description><link>https://arxiv.org/abs/2312.04535</link><guid isPermaLink="true">https://arxiv.org/abs/2312.04535</guid><pubDate>Thu, 07 Dec 2023 00:00:00 -0000</pubDate></item></channel></rss>